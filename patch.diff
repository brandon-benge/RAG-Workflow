diff --git a/scripts/quiz/master.py b/scripts/quiz/master.py
--- a/scripts/quiz/master.py
+++ b/scripts/quiz/master.py
@@ -246,7 +246,31 @@ def _parse_model_questions(raw_json: str, provider: str) -> List[Question]:
     return out
 
 
-# Providers class remains unchanged; copy from original generate_quiz.py
 class Providers:
+    def __init__(self, cfg: Config):
+        self.cfg = cfg
+        # Lazy import so the script can run without requests when not using Ollama
+        try:
+            import requests  # type: ignore
+            self._requests = requests
+        except Exception:
+            self._requests = None
+
+    def _dump_payload(self, prompt: str, payload: Dict[str, Any]) -> None:
+        # Best-effort debug dumps; ignore failures
+        try:
+            if self.cfg.dump_llm_payload:
+                Path(self.cfg.dump_llm_payload).write_text(prompt, encoding='utf-8')
+            if self.cfg.dump_ollama_payload:
+                Path(self.cfg.dump_ollama_payload).write_text(json.dumps(payload, indent=2), encoding='utf-8')
+        except Exception:
+            pass
+
+    def openai_questions(self, *args, **kwargs):
+        # This build path is RAG + Ollama only. Raise clearly if someone tries --ai.
+        raise RuntimeError(
+            'OpenAI provider not enabled in this build. '
+            'Use --ollama or implement openai_questions.'
+        )
 
     def ollama_questions(self, files: Dict[str, str], count: int, model: str,
                          token: str, recent_norm: List[str], temperature: float,
